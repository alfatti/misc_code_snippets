{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def two_combinations(vector):\n",
        "    \"\"\"\n",
        "    Convert an n-value vector to an n^2 2-dimensional vector representing all 2-combinations.\n",
        "\n",
        "    :param vector: An n-value vector (list or numpy array).\n",
        "    :return: An n^2 2-dimensional vector (2D numpy array).\n",
        "    \"\"\"\n",
        "    n = len(vector)\n",
        "    combination_vectors = []\n",
        "\n",
        "    for i in range(n):\n",
        "        for j in range(n):\n",
        "            combination_vectors.append([vector[i], vector[j]])\n",
        "\n",
        "    return np.array(combination_vectors)\n"
      ],
      "metadata": {
        "id": "rJtd7rV8Ub79"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import expon\n",
        "\n",
        "def simulate_markov_batch(Q, initial_states, times, num_simulations):\n",
        "    num_states = len(Q)\n",
        "    num_times = len(times)\n",
        "    states_at_times = np.zeros((num_simulations, num_times), dtype=int)\n",
        "\n",
        "    for sim in range(num_simulations):\n",
        "        state = initial_states\n",
        "        states_at_times[sim, 0] = state\n",
        "\n",
        "        for i in range(1, num_times):\n",
        "            current_time = times[i - 1]\n",
        "            end_time = times[i]\n",
        "            while current_time < end_time:\n",
        "                rate = -Q[state, state]\n",
        "                time_to_next = expon.rvs(scale = 1/rate)\n",
        "                #print(time_to_next)\n",
        "\n",
        "\n",
        "                current_time += time_to_next\n",
        "                if current_time < end_time:\n",
        "                    transition_probs = Q[state, :] / rate\n",
        "                    transition_probs[state] = 0\n",
        "                    state = np.random.choice(num_states, p=transition_probs)\n",
        "\n",
        "            states_at_times[sim, i] = state\n",
        "\n",
        "    return states_at_times\n",
        "\n",
        "\n",
        "\n",
        "def simulate_markov_batch_1(Q, initial_states, times, num_simulations , delta_t):\n",
        "    num_states = len(Q)\n",
        "    num_times = len(times)\n",
        "    states_at_times = np.zeros((num_simulations, num_times), dtype=int)\n",
        "\n",
        "    for sim in range(num_simulations):\n",
        "        state = initial_states\n",
        "        states_at_times[sim, 0] = state\n",
        "        q = Q * delta_t\n",
        "        I = np.identity(num_states)\n",
        "        p = np.zeros((1,num_states))\n",
        "        p[0,state] = 1\n",
        "        for i in range(1, num_times):\n",
        "          p = np.matmul(p,q+I)\n",
        "          #print(p)\n",
        "          state = np.random.choice(num_states, p=np.squeeze(p))\n",
        "          states_at_times[sim, i] = state\n",
        "\n",
        "    return states_at_times\n",
        "\n"
      ],
      "metadata": {
        "id": "RDyxIEL9aAAN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "expon.rvs( scale = 1/60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txuuXurXJGfC",
        "outputId": "ddc15747-39a3-41b7-806b-35553ddcc50f"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0003763514443176389"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(expon.rvs(size=18 , scale= 1/18) > 1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INjXb5oksSlC",
        "outputId": "a2fb31d6-c64b-4be0-ed3d-b189074b9ae1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "U8wRghq8WLKY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, Tensor\n",
        "import numpy as np\n",
        "from torch.nn import Module, Linear, BatchNorm1d, Tanh\n",
        "from numba import cuda\n",
        "from torch import autograd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\"\"\"\n",
        "In this section we have used code available online in : https://github.com/frankhan91/DeepBSDE\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class StochasticProcess:\n",
        "    \"\"\"\n",
        "    Base class for defining PDE related function.\n",
        "\n",
        "    Args:\n",
        "    eqn_config (dict): dictionary containing PDE configuration parameters\n",
        "\n",
        "    Attributes:\n",
        "    dim (int): dimensionality of the problem\n",
        "    total_time (float): total time horizon\n",
        "    num_time_interval (int): number of time steps\n",
        "    delta_t (float): time step size\n",
        "    sqrt_delta_t (float): square root of time step size\n",
        "    y_init (None): initial value of the function\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, eqn_config: dict):\n",
        "        self.dim = eqn_config['dim']\n",
        "        self.total_time = eqn_config['total_time']\n",
        "        self.num_time_interval = eqn_config['num_time_interval']\n",
        "        self.delta_t = self.total_time / self.num_time_interval\n",
        "        self.sqrt_delta_t = np.sqrt(self.delta_t)\n",
        "        self.y_init = None\n",
        "\n",
        "    def sample(self, num_sample: int) -> Tensor:\n",
        "        \"\"\"\n",
        "        Sample forward SDE.\n",
        "\n",
        "        Args:\n",
        "        num_sample (int): number of samples to generate\n",
        "\n",
        "        Returns:\n",
        "        Tensor: tensor of size [num_sample, dim+1] containing samples\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def r_u(self, t: float, x: Tensor, y: Tensor, z: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Interest rate in the PDE.\n",
        "\n",
        "        Args:\n",
        "        t (float): current time\n",
        "        x (Tensor): tensor of size [batch_size, dim] containing space coordinates\n",
        "        y (Tensor): tensor of size [batch_size, 1] containing function values\n",
        "        z (Tensor): tensor of size [batch_size, dim] containing gradients\n",
        "\n",
        "        Returns:\n",
        "        Tensor: tensor of size [batch_size, 1] containing generator values\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def h_z(self, t,x,y,z: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Function to compute H(z) in the PDE.\n",
        "\n",
        "        Args:\n",
        "        h (float): value of H function\n",
        "        z (Tensor): tensor of size [batch_size, dim] containing gradients\n",
        "\n",
        "        Returns:\n",
        "        Tensor: tensor of size [batch_size, dim] containing H(z)\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def terminal(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Terminal condition of the PDE.\n",
        "\n",
        "        Args:\n",
        "        t (float): current time\n",
        "        x (Tensor): tensor of size [batch_size, dim] containing space coordinates\n",
        "\n",
        "        Returns:\n",
        "        Tensor: tensor of size [batch_size, 1] containing terminal values\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "class RFQ(StochasticProcess):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "  eqn_config (dict): dictionary containing PDE configuration parameters\n",
        "  \"\"\"\n",
        "  def __init__(self, basic_config,specific_config):\n",
        "    super(RFQ, self).__init__(basic_config)\n",
        "\n",
        "    self.n_liqiudity_state  = specific_config['nls']\n",
        "    self.x_init = np.ones(self.dim) * specific_config['init']\n",
        "    self.lamdas = specific_config['lamdas']\n",
        "    self.lamda_initial_state = specific_config['init_state'] #integer\n",
        "    self.sigma = specific_config['sigma']\n",
        "    self.k = specific_config['k']\n",
        "    self.Q = specific_config['Q']\n",
        "\n",
        "  def sample(self, num_sample)-> tuple:\n",
        "    \"\"\"\n",
        "    Sample forward SDE.\n",
        "\n",
        "    Args:\n",
        "    num_sample (int): number of samples to generate\n",
        "\n",
        "    Returns:\n",
        "    tuple: tuple of two tensors: dw_sample of size [num_sample, dim, num_time_interval] and\n",
        "    x_sample of size [num_sample, dim, num_time_interval+1]\n",
        "    \"\"\"\n",
        "\n",
        "    dw_sample = np.random.normal(size=[num_sample,  self.num_time_interval]) * self.sqrt_delta_t\n",
        "    x_sample = np.zeros([num_sample, self.num_time_interval + 1])\n",
        "    x_sample[:, 0] = np.ones(num_sample) * self.x_init\n",
        "\n",
        "    select_Q = np.ones([num_sample, self.n_liqiudity_state**2  , self.n_liqiudity_state**2  ]) * np.expand_dims(np.exp(self.Q * self.delta_t), axis=0)\n",
        "    select_lamda = np.ones([num_sample, self.n_liqiudity_state**2 , 2]) * np.expand_dims(two_combinations(self.lamdas), axis=0)\n",
        "    #lamda_process = simulate_markov_batch(self.Q ,self.lamda_initial_state,np.array([i*self.delta_t for i in range(self.num_time_interval)]) ,num_sample )\n",
        "    lamda_process = simulate_markov_batch_1(self.Q ,self.lamda_initial_state,np.array([i*self.delta_t for i in range(self.num_time_interval)]) ,num_sample ,  self.delta_t)\n",
        "\n",
        "    ask_lamda = np.array([[self.lamdas[x // 2]for x in y] for y in  lamda_process ])\n",
        "    bid_lamda = np.array([[self.lamdas[x % 2]for x in y] for y in  lamda_process ])\n",
        "\n",
        "    for i in range(self.num_time_interval):\n",
        "\n",
        "        x_sample[:,  i + 1] =  x_sample[:,  i ]+ (ask_lamda[:, i] -  bid_lamda[:, i]) * self.k * self.delta_t + self.sigma * dw_sample[:, i]\n",
        "    return ask_lamda , bid_lamda, x_sample , lamda_process\n",
        "\n",
        "  def r_u(self, t, x, y, z)-> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Interest rate in the PDE.\n",
        "\n",
        "    Args:\n",
        "    t (float): current time\n",
        "    x (torch.Tensor): tensor of size [batch_size, dim] containing space coordinates\n",
        "    y (torch.Tensor): tensor of size [batch_size, 1] containing function values\n",
        "    z (torch.Tensor): tensor of size [batch_size, dim] containing gradients\n",
        "\n",
        "    Returns:\n",
        "    torch.Tensor: tensor of size [batch_size, 1] containing generator values\n",
        "    \"\"\"\n",
        "\n",
        "    return 0\n",
        "\n",
        "  def h_z(self,t,x,y,z)-> torch.Tensor:\n",
        "      \"\"\"\n",
        "      Function to compute $h^T Z$ in the PDE.\n",
        "\n",
        "      Args:\n",
        "      t (float): current time\n",
        "      x (torch.Tensor): tensor of size [batch_size, dim] containing space coordinates\n",
        "      y (torch.Tensor): tensor of size [batch_size, 1] containing function value\n",
        "      z (torch.Tensor): tensor of size [batch_size, dim] containing gradients\n",
        "\n",
        "      Returns:\n",
        "      torch.Tensor: tensor of size [batch_size, 1] containing H(z)\n",
        "      \"\"\"\n",
        "      return 0\n",
        "\n",
        "\n",
        "  def terminal(self,  x)-> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Terminal condition of the PDE.\n",
        "\n",
        "    Args:\n",
        "    t (float): current time\n",
        "    x (torch.Tensor): tensor of size [batch_size, dim] containing space coordinates\n",
        "\n",
        "    Returns:\n",
        "    torch.Tensor: tensor of size [batch_size, 1] containing terminal values\n",
        "    \"\"\"\n",
        "    return 0\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"A trading environment\"\"\"\n",
        "import gym\n",
        "from gym import spaces\n",
        "from gym.utils import seeding\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class BoudEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    trading environment;\n",
        "    \"\"\"\n",
        "\n",
        "    # trade_freq in unit of day, e.g 2: every 2 day; 0.5 twice a day;\n",
        "    def __init__(self,basic_config,specific_config, num_sim=100):\n",
        "\n",
        "        # simulated data: array of asset price, option price and delta paths (num_path x num_period)\n",
        "        # generate data now\n",
        "        self.sp = RFQ(basic_config,specific_config)\n",
        "\n",
        "\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "      return  self.sp.sample(batch_size)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ah-YINvcYww0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def weights_init_uniform_rule(m):\n",
        "        classname = m.__class__.__name__\n",
        "        # for every Linear layer in a model..\n",
        "        if classname.find('nn.Linear') != -1:\n",
        "            # get the number of the inputs\n",
        "            n = m.in_features\n",
        "            y = 1.0/np.sqrt(n)\n",
        "            m.weight.data.uniform_(-y, y)\n",
        "            m.bias.data.fill_(0)\n",
        "\n",
        "class RL_Net(nn.Module):\n",
        "    def __init__(self,INPUT_DIM,OUTPUT_DIM,HIDDEN_DIM , First = False):\n",
        "        super(RL_Net, self).__init__()\n",
        "        self.input_dim = INPUT_DIM\n",
        "        self.output_dim = OUTPUT_DIM\n",
        "        self.hidden_dim = HIDDEN_DIM\n",
        "        self.is_first = First\n",
        "        current_dim = self.input_dim\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.bn = nn.ModuleList()\n",
        "        self.droupout = nn.ModuleList() #drop out layer for regularization\n",
        "        for hdim in self.hidden_dim:\n",
        "            self.layers.append(nn.Linear(int(current_dim), int(hdim)))\n",
        "            self.bn.append(nn.BatchNorm1d(int(hdim)))\n",
        "            self.droupout.append(nn.Dropout(0.25)) # add a dropout layer\n",
        "            current_dim = hdim\n",
        "        self.layers.append(nn.Linear(int(current_dim), int(self.output_dim)))\n",
        "    def forward(self, x):\n",
        "        for i, layer in enumerate(self.layers[:-1]):\n",
        "            x = layer(x)\n",
        "            if self.is_first == False:\n",
        "              x = self.bn[i](x)\n",
        "            x = F.tanh(x)\n",
        "        out = self.layers[-1](x)\n",
        "        return out"
      ],
      "metadata": {
        "id": "420W42w8CyC9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scurve(delta , alpha , beta , delta_0):\n",
        "  return 1/(1 + torch.exp(alpha + beta/delta_0 * delta))"
      ],
      "metadata": {
        "id": "_ojuq8fRE5qu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def utility(ask_delta , bid_delta, q , z , ask_lamda, bid_lamda , gamma , alpha, beta, delta_0 , kappa , delta_t , sigma):\n",
        "  t = ask_delta.shape[1]\n",
        "\n",
        "  res = 0\n",
        "  for i in range(t):\n",
        "    res += (z*ask_delta[:,i] * ask_lamda[:,i]* scurve(ask_delta[:,i],alpha , beta, delta_0) + z*bid_delta[:,i] * bid_lamda[:,i]* scurve(bid_delta[:,i],alpha , beta, delta_0) + kappa*(ask_lamda[:,i] -bid_lamda[:,i] ) * q[:,i] - gamma/2 * q[:,i]**2 * sigma**2) * delta_t\n",
        "  return torch.mean(res)\n"
      ],
      "metadata": {
        "id": "FYD_v-eB1TCh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fare_price(env,gamma,delta_0, z ,alpha, beta , batch_size = 200 , epoch= 100, device = torch.device('cpu')):\n",
        "  ask_models = []\n",
        "  bid_models = []\n",
        "  i = 0\n",
        "  n_model = env.sp.num_time_interval\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  while i <= n_model-1:\n",
        "      f= False\n",
        "\n",
        "      model = RL_Net(4,1,[10,10,20,10]).to(device)\n",
        "\n",
        "\n",
        "\n",
        "      model.apply(weights_init_uniform_rule)\n",
        "      ask_models.append(model)\n",
        "\n",
        "      model = RL_Net(4 ,1,[10,10,20,10]).to(device)\n",
        "\n",
        "\n",
        "\n",
        "      model.apply(weights_init_uniform_rule)\n",
        "      bid_models.append(model)\n",
        "      i += 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  optimizer_bid= torch.optim.Adam((par for model in bid_models\n",
        "                    for par in model.parameters()),\n",
        "                    lr=0.001, betas=(0.9, 0.99))\n",
        "  optimizer_ask= torch.optim.Adam((par for model in ask_models\n",
        "                    for par in model.parameters()),\n",
        "                    lr=0.001, betas=(0.9, 0.99))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  for _ in range(epoch):\n",
        "    ask_lamda, bid_lamda,prices,_ = env.sample(batch_size)\n",
        "    ask_lamda = torch.tensor(ask_lamda).to(torch.float32)\n",
        "    bid_lamda = torch.tensor(bid_lamda).to(torch.float32)\n",
        "    prices = torch.tensor(prices).to(torch.float32)\n",
        "    q = torch.zeros((batch_size ,env.sp.num_time_interval + 1))\n",
        "    ask_delta = torch.zeros((batch_size ,env.sp.num_time_interval))\n",
        "    bid_delta = torch.zeros((batch_size ,env.sp.num_time_interval))\n",
        "    optimizer_ask.zero_grad()\n",
        "    optimizer_bid.zero_grad()\n",
        "    for t in range(env.sp.num_time_interval):\n",
        "\n",
        "      q[:, t+1] = z*(bid_lamda[:,t] - ask_lamda[:,t])\n",
        "      input = torch.cat((torch.ones((batch_size ,1 ))*t * env.sp.delta_t ,q[:, t:t+1]  , ask_lamda[:,t:t+1] , bid_lamda[:,t:t+1]), dim=1)\n",
        "      ask_delta[:,t:t+1] = ask_models[t](input)\n",
        "      bid_delta[:,t:t+1] = bid_models[t](input)\n",
        "\n",
        "    loss = -1 * utility(ask_delta,bid_delta,q,z,ask_lamda, bid_lamda,gamma,alpha,beta,delta_0,env.sp.k,env.sp.delta_t,env.sp.sigma)\n",
        "    print(-1 * loss)\n",
        "    loss.backward()\n",
        "    optimizer_ask.step()\n",
        "    optimizer_bid.step()\n",
        "\n"
      ],
      "metadata": {
        "id": "TQpR01dZ5G6u"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q = np.array([[-14.01, 4.37,4.37,5.27] , [19.32, -60.91,12.54,29.05] , [19.32,12.54,-60.91,29.05] , [23.67,15.00,15.00,-53.67]])"
      ],
      "metadata": {
        "id": "wSxmesJAE4RU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "basic = {'dim' : 1 , 'total_time' : 0.25 , 'num_time_interval': 90 }\n",
        "specific = {'init':103.593 , 'sigma':0.1839 ,  'nls' :2 , 'lamdas' : np.array([10.83, 73.03]) /10, 'init_state':1,'k' : 2.29 , 'Q': Q}\n",
        "\n",
        "env = BoudEnv(basic , specific)"
      ],
      "metadata": {
        "id": "7V4kLdq2DNwG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0089b6ec-1c04-4439-ef55-1676fba8a30d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.sp.delta_t"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LA4s1XWqWKyq",
        "outputId": "862f18a8-7eaf-4c00-fb64-32b866c874a5"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.002777777777777778"
            ]
          },
          "metadata": {},
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pmwU3vY2aYpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.sample(1)[3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVMRNJblr1xq",
        "outputId": "fb0463ed-99ea-45ad-855b-eb0bea948ae9"
      },
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 1, 1, 2, 3, 1, 1, 3, 0, 2, 3, 2, 1, 0, 0, 0, 0, 0, 3, 1, 3, 2,\n",
              "        0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 3, 0, 0, 0, 1, 0, 0, 0, 3,\n",
              "        2, 3, 1, 0, 0, 0, 0, 3, 1, 0, 0, 3, 0, 1, 0, 3, 0, 0, 0, 2, 0, 0,\n",
              "        0, 0, 3, 1, 2, 1, 0, 0, 3, 0, 0, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 2,\n",
              "        3, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 208
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(np.sum((env.sample(4000)[0] - env.sample(4000)[1]) *env.sp.delta_t,axis = 1))"
      ],
      "metadata": {
        "id": "N5y6kOAkFxQs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14479fcb-0e47-4e7d-fae6-0e299bba68ac"
      },
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.10340318055555554"
            ]
          },
          "metadata": {},
          "execution_count": 214
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fare_price(env,0.0005 , 0.09,1,-0.7,3.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyiMmxGRGq24",
        "outputId": "2ef83d82-59e8-473f-c1ee-84dc06afa159"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(-0.8738, grad_fn=<MulBackward0>)\n",
            "tensor(-0.8084, grad_fn=<MulBackward0>)\n",
            "tensor(-0.7296, grad_fn=<MulBackward0>)\n",
            "tensor(-0.6856, grad_fn=<MulBackward0>)\n",
            "tensor(-0.6469, grad_fn=<MulBackward0>)\n",
            "tensor(-0.6495, grad_fn=<MulBackward0>)\n",
            "tensor(-0.7003, grad_fn=<MulBackward0>)\n",
            "tensor(-0.6408, grad_fn=<MulBackward0>)\n",
            "tensor(-0.6808, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5453, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5987, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5880, grad_fn=<MulBackward0>)\n",
            "tensor(-0.6473, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5688, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5561, grad_fn=<MulBackward0>)\n",
            "tensor(-0.6302, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5610, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5984, grad_fn=<MulBackward0>)\n",
            "tensor(-0.6236, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5380, grad_fn=<MulBackward0>)\n",
            "tensor(-0.6752, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5333, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5649, grad_fn=<MulBackward0>)\n",
            "tensor(-0.6415, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5550, grad_fn=<MulBackward0>)\n",
            "tensor(-0.6008, grad_fn=<MulBackward0>)\n",
            "tensor(-0.4951, grad_fn=<MulBackward0>)\n",
            "tensor(-0.4860, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5805, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5866, grad_fn=<MulBackward0>)\n",
            "tensor(-0.4828, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5606, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5285, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5225, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5310, grad_fn=<MulBackward0>)\n",
            "tensor(-0.6369, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5647, grad_fn=<MulBackward0>)\n",
            "tensor(-0.4968, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5583, grad_fn=<MulBackward0>)\n",
            "tensor(-0.6226, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5511, grad_fn=<MulBackward0>)\n",
            "tensor(-0.4703, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5971, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5851, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5113, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5232, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5837, grad_fn=<MulBackward0>)\n",
            "tensor(-0.4732, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5819, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5473, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5444, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5326, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5415, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5700, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5683, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5977, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5230, grad_fn=<MulBackward0>)\n",
            "tensor(-0.4588, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5662, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5213, grad_fn=<MulBackward0>)\n",
            "tensor(-0.4999, grad_fn=<MulBackward0>)\n",
            "tensor(-0.6006, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5075, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5101, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5280, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5920, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5519, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5761, grad_fn=<MulBackward0>)\n",
            "tensor(-0.4430, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5277, grad_fn=<MulBackward0>)\n",
            "tensor(-0.4889, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5101, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5031, grad_fn=<MulBackward0>)\n",
            "tensor(-0.6115, grad_fn=<MulBackward0>)\n",
            "tensor(-0.4910, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5154, grad_fn=<MulBackward0>)\n",
            "tensor(-0.6008, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5981, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5635, grad_fn=<MulBackward0>)\n",
            "tensor(-0.4831, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5158, grad_fn=<MulBackward0>)\n",
            "tensor(-0.6098, grad_fn=<MulBackward0>)\n",
            "tensor(-0.4805, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5706, grad_fn=<MulBackward0>)\n",
            "tensor(-0.4927, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5542, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5016, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5491, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5323, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5505, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5615, grad_fn=<MulBackward0>)\n",
            "tensor(-0.4773, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5691, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5407, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5170, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5271, grad_fn=<MulBackward0>)\n",
            "tensor(-0.4996, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5512, grad_fn=<MulBackward0>)\n",
            "tensor(-0.6235, grad_fn=<MulBackward0>)\n",
            "tensor(-0.5185, grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    }
  ]
}